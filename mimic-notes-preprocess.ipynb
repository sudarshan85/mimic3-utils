{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC Notes Pre-Processing\n",
    "\n",
    "Pre-processing MIMIC notes for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a list of redacted items with an example and the replacement token. Replacement tokens are changeable. Check `preprocess_notes.py` for more details.\n",
    "\n",
    "Redacted items:\n",
    "* [x] First Name: `[**First Name (Titles) 137**]`, `xxname`\n",
    "* [x] Last Name: `[**Last Name (Titles) **]`, `xxln`\n",
    "* [x] Initials: `[**Initials (NamePattern4) **]`, `xxinit`\n",
    "* [x] Name: `[**Name (NI) **]`, `xxname`\n",
    "* [x] Doctor First Name: `[**Doctor First Name 1266**]`, `xxdocfn`\n",
    "* [x] Doctor Last Name: `[**Doctor Last Name 1266**]`, `xxdocln`\n",
    "* [x] Known Last Name: `[**Known lastname 658**]`, `xxln`\n",
    "* [x] Hospital: `[**Hospital1 **]`, `xxhosp`\n",
    "* [x] Hospital Unit Name: `**Hospital Unit Name 10**`, `xxhosp`\n",
    "* [x] Company: `[**Company 12924**]`, `xxwork`\n",
    "* [x] University/College: `[**University/College **]`, `xxwork`\n",
    "* [x] Date of format YYYY-M-DD: `[**2112-4-18**]`, `xxdate`\n",
    "* [x] Year: `[**Year (4 digits) **]`, `xxyear`\n",
    "* [x] Year YYYY format: `[**2119**]`, `xxyear` - I use a regex `\\b\\d{4}\\b` that will match **any** 4 digits which might be problematic, but for the most part 4 digits by itself seems to indicate a year.\n",
    "* [x] Date of format M-DD: `[**6-12**]`, `[**12/2151**]`, `xxmmdd`\n",
    "* [x] Month/Day: `[**Month/Day (2) 509**]`, `xxmmdd`\n",
    "* [x] Month (only): `[**Month (only) 51**]`, `xxmonth`\n",
    "* [x] Holiday: `[**Holiday 3470**]`, `xxhols`\n",
    "* [x] Date Range: `[**Date range (1) 7610**]`, `xxdtrnge`\n",
    "* [x] Country: `[**Country 9958**]`, `xxcntry`\n",
    "* [x] State: `[**State 3283**]`, `xxstate`\n",
    "* [x] Location: `**Location (un) 2432**`, `xxloc`\n",
    "* [x] Telephone/Fax: `[**Telephone/Fax (3) 8049**]`, `xxph`\n",
    "* [x] Clip Number: `[**Clip Number (Radiology) 29923**]`, `xxradclip`\n",
    "* [x] Pager Numeric Identifier: `[**Numeric Identifier 6403**]`, `xxpager`\n",
    "* [x] Pager Number: `[**Pager number 13866**]`, `xxpager`\n",
    "* [x] Social Security Number: `[**Security Number 10198**]`, `xxssn`\n",
    "* [x] Serial Number: `[**Serial Number 3567**]`, `xxsno`\n",
    "* [x] Medical Record Number: `[**Medical Record Number **]`, `xxmrno`\n",
    "* [x] Provider Number: `[**Provider Number 12521**]`, `xxpno`\n",
    "* [x] Age over 90: `[**Age over 90 **]`, `xxage90`\n",
    "* [x] Contact Info: `[**Contact Info **]`, `xxcontact`\n",
    "* [x] Job Number: `[**Job Number **]`, `xxjobno`\n",
    "* [x] Dictator Number: `[**Dictator Info **]`, `xxdict`\n",
    "* [x] Pharmacy MD Number/MD number: `[**Pharmacy MD Number **]`, `xxmdno`\n",
    "* [x] Time: `12:52 PM`, split into 6 segments by the hour and replace with the following tokens: `midnight, dawn, forenoon, afternoon, dusk, night`\n",
    "* 2-digit Numbers: `[** 84 **]`, `xx2digit`\n",
    "* 3-digit Numbers: `[** 834 **]`, `xx3digit`\n",
    "* Wardname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`886` notes are marked incorrect with `iserror` flag set to 1. Thus, there are total of `2,082,294` notes. I have set up a `view` called `correctnotes` in the database, which only includese the correct notes. All the data I grab is from that `view`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softlink `ln -s` your data path to a `data` variable in the current folder. That way we don't need to change the path in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_notes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab Data from MIMIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the data is grabbed from the MIMIC database. Data can also be grabbed from other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T23:05:14.536531Z",
     "start_time": "2018-03-18T23:04:17.500997Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cats = pd.read_csv('note_categories.csv')\n",
    "max_limit = 10\n",
    "\n",
    "queries = []\n",
    "for category, n_notes in zip(cats['category'], cats['number_of_notes']):\n",
    "    limit = min(max_limit, n_notes) if max_limit > 0 else n_notes\n",
    "    if limit == max_limit:\n",
    "        q = f\"\"\"\n",
    "        select * from correctnotes where category=\\'{category}\\' order by random() limit {limit};\n",
    "        \"\"\"\n",
    "    else:\n",
    "        q = f\"\"\"\n",
    "        select * from correctnotes where category=\\'{category}\\';\n",
    "        \"\"\"\n",
    "    queries.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-18T23:05:14.536531Z",
     "start_time": "2018-03-18T23:04:17.500997Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 10)\n",
      "CPU times: user 57.3 ms, sys: 0 ns, total: 57.3 ms\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfs = []\n",
    "\n",
    "con = psycopg2.connect(dbname='mimic', user='sudarshan', host='/var/run/postgresql')\n",
    "for q in queries:\n",
    "    df = pd.read_sql_query(q, con)\n",
    "    dfs.append(df)\n",
    "con.close()\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Notes File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv(PATH/'NOTEEVENTS.csv.gz')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 9)\n"
     ]
    }
   ],
   "source": [
    "df.columns = map(str.lower, df.columns)\n",
    "df.set_index('row_id', inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat1 = re.compile(r'-?\\byears? ?-?old\\b|\\by(?:o|r)*[ ./-]*o(?:ld)?\\b')\n",
    "pat2 = re.compile(r'(\\d+)\\s*(year\\s*old|y.\\s*o.|yo|year\\s*old|year-old|-year-old|-year old)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patm,patw = re.compile(r'\\b(male|man|m|M)\\b', re.IGNORECASE),re.compile(r'\\b(woman|female|f|F)\\b')\n",
    "patm,patw = re.compile(r'\\b(male|man|m|M)(?!\\S)\\b', re.IGNORECASE),re.compile(r'\\b(female|woman|f|F)(?!\\S)\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[random.randint(0, len(df))]['text']\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patm.findall(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patw.findall(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat1.findall(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2.findall(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the number of notes match the actual number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['category', 'text']].groupby(['category']).agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 170 ms, sys: 0 ns, total: 170 ms\n",
      "Wall time: 169 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['proc_text'] = df['text'].apply(preprocess_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE:\n",
      "BEDSIDE SWALLOWING EVALUATION:\n",
      "HISTORY:\n",
      "Thank you for consulting on this 66 yo female with hx of prior\n",
      "strokes and PD admitted on xxdate from OSH with confusion and\n",
      "weakness. CT scan showed bilateral hypodensities. patient s daughter\n",
      "reported recent hospitalization ~1 week PTA for possible seizure\n",
      "w/ d/c home. patient was transferred from Far 11 to the ICU on xxmmdd\n",
      "for vomiting x 2, left sided weakness and seizure activity. Head\n",
      "CT showed new right sided infarct. patient was found with moderate\n",
      "stenosis and is awaiting CEA. We were consulted to evaluate for\n",
      "oral and pharyngeal dysphagia.\n",
      "Other PMH includes asthma, COPD, CAD, HTN and MI\n",
      "EVALUATION:\n",
      "The examination was performed while the patient was seated\n",
      "upright in the bed in the SICU.\n",
      "Cognition, language, speech, voice:\n",
      "patient was lethargic, but did open her eyes on command. patient was seen\n",
      "following PT in attempts to have her most awake for the\n",
      "evaluation. Language could not be assessed, as her spontaneous\n",
      "output was minimal. She did not respond to any orientation\n",
      "question spontaneously, but did state she was in the hospital\n",
      "when given 2 choices. She followed ~50% of basic commands. It was\n",
      "difficult to determine what was xxmmdd difficulty comprehending vs\n",
      "lethargy.\n",
      "Teeth: average condition\n",
      "Secretions: appeared wfl in the oral cavity\n",
      "ORAL MOTOR EXAM:\n",
      "Limited xxmmdd patient positioning in bed, but appeared reduced movement\n",
      "bilaterally. I was unable to assess tongue strength or ROM. patient \n",
      "bit down when attempting to assess palatal elevation or gag.\n",
      "SWALLOWING ASSESSMENT:\n",
      "The patient was seen with ice chips, thin liquids (tsp, straw), nectar\n",
      "thick liquids (tsp, straw) and purees. She did accept all\n",
      "boluses, but appeared to all asleep with food in her mouth x2.\n",
      "She did not require suctioning as she did swallow with cuing for\n",
      "all boluses given. She did not have any overt coughing, throat\n",
      "clearing or changes in vocal quality. She did not respond to\n",
      "questions re: sensation of aspiration or residue, but O2 SATs\n",
      "remained stable. patient did not allow me to palpate.\n",
      "SUMMARY / IMPRESSION:\n",
      "Mrs. xxln appeared extremely lethargic, although MD reports\n",
      "there may be a psychological / volitional component to her\n",
      "limited participation. While there were no overt signs of\n",
      "aspiration, she is not safe to take anything PO, as she appeared\n",
      "to fall asleep with food in her mouth, putting her at risk to\n",
      "aspirate. MD is requesting a repeat evaluation tomorrow, but PO\n",
      "intake will likely not be safe until her ability to participate,\n",
      "whether voluntary or involuntary improves. Even if she starts\n",
      "accepting small amounts, it is unlikely she will meet nutritional\n",
      "needs and she will likely benefit from placement of a Dobbhoff.\n",
      "This swallowing pattern correlates to a Dysphagia Outcome\n",
      "Severity Scale (DOSS) rating of 1, NPO.\n",
      "RECOMMENDATIONS:\n",
      "1. Suggest patient remain NPO.\n",
      "2. Recommend placement of a Dobbhoff for alternate means of\n",
      "nutrition, hydration and medication. patient will require supplemental\n",
      "nutrition as her intake will likely be limited for some time.\n",
      "3. We will continue to follow her during her admission.\n",
      "These recommendations were shared with the patient, nurse and\n",
      "medical team.\n",
      "____________________________________\n",
      "xxfn xxln, M.S., CCC-SLP\n",
      "Pager #xxpager\n",
      "Face time: 1:05-1:25\n",
      "Total time:   60 minutes\n",
      "   [BUTTON Input] (not implemented)_____\n",
      "   13:31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[random.randint(0, len(df))]['proc_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH/'preprocessed_noteevents.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets for Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow the FastAI language modeling lesson, I've created a subset of the original dataframe to sample for the datasets. In particular, I've included the `description` and `preprocessed_text` fields in the datasets. The `description` column is composed of free-text and has `3840` unique descriptions. I consider the description as a unique `field` which will be marked as such during tokenization as done in the FastAI library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({'proc_text': df['proc_text'], 'category': df['category'], 'description': df['description'], 'labels': [0]*len(df)},\\\n",
    "                      columns=['labels', 'category', 'description', 'proc_text'])\n",
    "sub_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just do a train/test split on the entire dataset for getting a 90/10 training and testing dataset. However, I would like the train/test set have a 90%/10% split in **each category**. So I chose to iterate over each entry of the `category` column and create masks to split data with a 90/10 split for training and testing so that I grab 10% of texts in each category for testing instead of a global 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed for reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "dfs = [sub_df.loc[df['category'] == c] for c in sub_df['category'].unique()]\n",
    "msks = [np.random.rand(len(d)) < 0.9 for d in dfs]\n",
    "\n",
    "train_dfs = [None] * len(dfs)\n",
    "val_dfs = [None] * len(dfs)\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    idf = dfs[i]\n",
    "    mask = msks[i]\n",
    "    train_dfs[i] = idf[mask]\n",
    "    val_dfs[i] = idf[~mask]\n",
    "    \n",
    "train_df = pd.concat(train_dfs)\n",
    "val_df = pd.concat(val_dfs)\n",
    "\n",
    "print(len(train_df), (len(df) - len(df)//10), len(train_df)-(len(df) - len(df)//10))\n",
    "print(len(val_df), (len(df)//10), len(val_df)-(len(df)//10))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check the aggregate count for each category over the 3 dataframes. Then write the `train` and `val` dataframes to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[['category', 'proc_text']].groupby(['category']).agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['category', 'proc_text']].groupby(['category']).agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df[['category', 'proc_text']].groupby(['category']).agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['labels', 'description', 'proc_text']].to_csv(PATH/'train.csv', header=False, index=False)\n",
    "val_df[['labels', 'description', 'proc_text']].to_csv(PATH/'test.csv', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
